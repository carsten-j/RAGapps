{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG application for Q&A with PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG is short for Retrieval-Augmented Generation and the term was coined in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf).\n",
    "\n",
    "The architecture of the proposed model was:\n",
    "\n",
    "<img src=\"images/rag-architecture.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __PDF Extraction and Preprocessing__  \n",
    "   • Extract Text: Use libraries like PyPDFLoader, PyPDF2, pdfplumber, or similar tools to extract the text content from the PDF file.  \n",
    "   • Clean and Preprocess: Remove unnecessary formatting, fix encoding issues, and possibly normalize the text (e.g., lowercasing, punctuation handling).  \n",
    "   • Document Segmentation: Depending on your PDF’s structure, you might want to segment it by chapters, sections, or pages if needed.\n",
    "\n",
    "2. __Chunking the Document__  \n",
    "   • Define Chunk Size: Split the extracted text into manageable chunks (e.g., paragraphs or fixed-size windows) so that each piece can be meaningfully processed.  \n",
    "   • Overlap Chunks: Optionally use overlapping windows to ensure smooth context transitions between chunks, which helps when a concept spans multiple chunks.\n",
    "\n",
    "3. __Creating Embeddings for the Text Chunks__  \n",
    "   • Choose an Embedding Model: Use a state-of-the-art embedding model (e.g., OpenAI’s embedding APIs, Sentence Transformers, etc.) that maps text chunks to high-dimensional vectors.  \n",
    "   • Generate Embeddings: Iterate over the chunks and compute their embeddings. This turns each text snippet into a vector which captures semantic meaning.\n",
    "\n",
    "4. __Building a Vector Index__\n",
    "   • Select a Vector Store: Use libraries such as Qdrant, Chroma, Faiss or Pinecone to store and index your embeddings.  \n",
    "   • Insert Embeddings: Store each vector along with metadata (like the chunk text, source page, or document section) for quick retrieval later on.\n",
    "\n",
    "5. __Setting Up the Retrieval Mechanism__  \n",
    "   • Query Embedding: When a user submits a question, embed the question using the same embedding model.  \n",
    "   • Similarity Search: Query the vector index to retrieve the top-n most similar text chunks based on the question’s embedding.  \n",
    "   • Relevance Ranking: Optionally rerank or verify retrieved passages to ensure they are the most contextually appropriate.\n",
    "\n",
    "6. __Constructing the RAG Pipeline__  \n",
    "   • Context Combination: Concatenate the retrieved chunks into a context prompt or pass them as additional inputs to the LLM.  \n",
    "   • Prompt Engineering: Craft a prompt that combines the user’s question with the retrieved context. Ensure the prompt instructs the LLM to use the provided evidence to answer the query.  \n",
    "   • LLM Query: Use an LLM (like GPT-4) to generate the final answer based on both the question and the supporting context from the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is based on [LangChain](https://www.langchain.com/) which is a composable framework to build with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download source document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the famous \"Attention Is All You Need\" paper as source document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://arxiv.org/pdf/1706.03762\n",
    "# !mv 1706.03762 PDFs/attention.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"I am reading the 'Attention is all you need'. \"\n",
    "query1 = prefix + \"What does the sentence in figure 5 say?\"  # Image\n",
    "query2 = prefix + \"What is the BLEU score of the big transformer model?\"  # Table\n",
    "query3 = prefix + \"How many GPU's were the models trained on?\"  # Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to section 5.3 of the original \"Attention Is All You Need\" paper, the authors trained their Transformer models on a single machine equipped with 8 NVIDIA P100 GPUs.\n"
     ]
    }
   ],
   "source": [
    "llm_openai = ChatOpenAI(model=\"o1\")\n",
    "\n",
    "chain_openai = prompt | llm_openai\n",
    "\n",
    "response_openai = chain_openai.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=query3,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_openai.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how many GPUs were used when training the models in \"Attention is All You Need.\" I remember that the paper was quite influential and it introduced the Transformer model. But I'm not exactly sure about the specifics of hardware usage.\n",
      "\n",
      "First, I think the original setup involved multiple GPUs because training large models can be computationally intensive. The idea must have been to distribute the workload across several machines to speed things up. I recall that each GPU might have had a certain number of chips or cores. Maybe around 8-16? That seems plausible for modern hardware.\n",
      "\n",
      "I also remember something about data distribution. The text wasn't just training one model but multiple models simultaneously. So, if there were four GPUs in total, each would handle two different tasks. This parallel processing should have made the training faster by utilizing all available resources effectively.\n",
      "\n",
      "Wait, I think the original setup used four GPUs. Each had eight chips or cores. That makes sense because having more than one GPU can significantly reduce the time needed for training large models. So, combining multiple GPUs would make the process much quicker and more efficient.\n",
      "\n",
      "Putting it all together, the models were trained on four GPUs. Each GPU probably had eight chips or cores, allowing them to handle two different tasks simultaneously. This setup was a smart way to leverage parallel computing power, making the training of large language models feasible within reasonable time frames.\n",
      "</think>\n",
      "\n",
      "The models in \"Attention is All You Need\" were trained using four GPUs. Each GPU featured eight chips or cores, enabling the simultaneous processing of two tasks per machine. This parallel computing approach significantly accelerated the training process by efficiently utilizing all available resources.\n"
     ]
    }
   ],
   "source": [
    "llm_deepseek = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", temperature=0, base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "chain_deepseek = prompt | llm_deepseek\n",
    "\n",
    "response_deepseek = chain_deepseek.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=query3,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_deepseek.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# loader = PyPDFDirectoryLoader(\"PDFs/\")\n",
    "loader = PyPDFLoader(\"./PDFs/attention.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document corresponds to one page in a PDF file. Let us explore the content of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz \n"
     ]
    }
   ],
   "source": [
    "print(f\"{documents[0].page_content[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './PDFs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting text can be anything from as easy as this simple example or as complex as you want it to be. It is much harder if you want to preserve metadata as chapters, sections etc. Extracting text from tables is not easy and it is even harder with figures and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text splitter\n",
    "\n",
    "Split each page into smaller chunks. \n",
    "\n",
    "add_start_index=True ensures meta data is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "# https://python.langchain.com/docs/how_to/split_by_token/\n",
    "\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     model_name=\"gpt-4\",\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     add_start_index=True\n",
    "# )\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './PDFs/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1610}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/qdrant/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 768\n",
      "\n",
      "first 5 elements in embedding vector: \n",
      "[0.00345193431712687, 0.01597711443901062, -0.013028663583099842, 0.0009539231541566551, -0.051165636628866196]\n"
     ]
    }
   ],
   "source": [
    "vector = embeddings.embed_query(all_splits[0].page_content)\n",
    "\n",
    "print(f\"Generated vectors of length {len(vector)}\\n\")\n",
    "print(f\"first 5 elements in embedding vector: \\n{vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"attention\"\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "collection_exists = client.collection_exists(collection_name=collection_name)\n",
    "\n",
    "if not collection_exists:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add embeddings to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not collection_exists:\n",
    "    print(\"Adding documents to the collection\")\n",
    "    ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate = d−0.5\n",
      "model · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps = 4000.\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './PDFs/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'start_index': 2384, '_id': 'fdef2efe-aca6-4e93-8da0-76565b925e9f', '_collection_name': 'attention'}\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query3)\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Q&A with LLM using vector database as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/qdrant/.venv/lib/python3.12/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how many GPUs were used in the models described in the 'Attention is all you need' paper. The user provided a context with some information about the setup. Let me read through it again.\n",
      "\n",
      "The context says that they trained their models on one machine with 8 NVIDIA P100 GPUs. So, each model was run on 8 of those GPUs. That makes sense because when I look at other parts of the text, like the optimizer and regularization sections, they mention using Adam optimizer and specific learning rate schedules. But for the number of GPUs used per model, it's clearly stated as 8.\n",
      "\n",
      "I don't see any conflicting information here. The context is pretty straightforward. It just mentions that each model was trained on one machine with 8 P100 GPUs. So, I think the answer is simply that they used 8 GPUs per model.\n",
      "</think>\n",
      "\n",
      "The models were trained on one machine with 8 NVIDIA P100 GPUs.\n"
     ]
    }
   ],
   "source": [
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_deepseek\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(query3)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
