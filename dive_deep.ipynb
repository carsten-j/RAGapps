{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens, Tokenizer, and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokens\n",
    "* __Definition__ : A token is a piece of text that is treated as a single unit by a language model.\n",
    "* __Examples__ : Tokens can be individual words, word segments, punctuation marks, or special markers (like [CLS] or [SEP] in some model architectures).\n",
    "* __Purpose__ : Language models work fundamentally on sequences of tokens. The first step in processing text is always tokenization—splitting text into tokens that the model “understands.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Tokenizer\n",
    "Definition : A tokenizer is a function (or a class) that takes raw text (a string) and outputs a list of tokens or numerical IDs representing those tokens.\n",
    "Mechanism : Different tokenizers use different approaches:\n",
    "Word-based tokenizers : The text is split on spaces and punctuation.\n",
    "Subword/BPE (Byte-Pair Encoding) tokenizers : The text is segmented into frequent subwords. For example, “embedding” might be split into “em”, “bed”, and “ding.”\n",
    "Character-based tokenizers : Every single character may become a token.\n",
    "Output : Typically, a tokenizer maps tokens to integer indices (IDs). For example, the word “Hello” might be token ID 440 in a certain vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Embeddings\n",
    "Definition : An embedding is a dense numerical vector that captures semantic or contextual information about a token (or an entire sequence).\n",
    "Model Output : When you pass your tokenized input into a language model (e.g., BERT, GPT), the first layer of the model converts each token ID into an embedding, which is typically a vector of floating-point numbers.\n",
    "Dimension : Embeddings often have dimensions in the hundreds or even thousands (e.g., 768 or 1024).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Putting It All Together\n",
    "You provide raw text (e.g., \"Hello, this is a test.\").\n",
    "The tokenizer splits this text into tokens and assigns IDs (e.g., [101, 7592, 1010, 2023, ...] for a BERT-based model).\n",
    "The model takes these token IDs and looks up or learns an corresponding embedding vector for each token.\n",
    "Higher layers of the model transform or refine these embeddings. Ultimately, these vectors are used in tasks such as classification, language generation, or other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens and Tokenizers in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's us get a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "# model = SentenceTransformer(\"bert-base-cased\", device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use the model's tokenizer. A tokenizer splits a sentence into tokens that can be represented by its vocabulary.\n",
    "\n",
    "This process is also known as encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 28988, 7997, 2094, 1037, 2146, 7997, 2778, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = tokenizer([\"biker biked a long bike tour\"])\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'biker', 'bike', '##d', 'a', 'long', 'bike', 'tour', '[SEP]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the process of converting token ids to text is called decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] biker biked a long bike tour [SEP]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just one example of a tokenizer. \n",
    "\n",
    "Can you think of more ways to split a text into tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Tokenization\n",
    "* Character Tokenization\n",
    "* Subword Tokenization\n",
    "    * Byte Pair Encoding (BPE)\n",
    "    * WordPiece\n",
    "    * SentencePiece\n",
    "    * Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need an embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._first_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer consists of multiple stack modules. Tokens are an input\n",
    "# of the first one, so we can ignore the rest.\n",
    "first_module = model._first_module()\n",
    "\n",
    "tokenizer = first_module.tokenizer\n",
    "embeddings = first_module.auto_model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"vector search optimization\"\n",
    "second_sentence = \"how do I use vector search optimization\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tokenize both texts\n",
    "    first_tokens = model.tokenize([first_sentence])\n",
    "    second_tokens = model.tokenize([second_sentence])\n",
    "\n",
    "    # Get the corresponding embeddings\n",
    "    first_embeddings = embeddings.word_embeddings(first_tokens[\"input_ids\"].to(device))\n",
    "    second_embeddings = embeddings.word_embeddings(\n",
    "        second_tokens[\"input_ids\"].to(device)\n",
    "    )\n",
    "\n",
    "first_embeddings.shape, second_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = (\n",
    "    util.cos_sim(first_embeddings.squeeze(), second_embeddings.squeeze()).cpu().numpy()\n",
    ")  # Move the tensor to the CPU and convert to a NumPy array\n",
    "\n",
    "px.imshow(\n",
    "    distances,\n",
    "    x=model.tokenizer.convert_ids_to_tokens(second_tokens[\"input_ids\"][0]),\n",
    "    y=model.tokenizer.convert_ids_to_tokens(first_tokens[\"input_ids\"][0]),\n",
    "    text_auto=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Visualizing the input embeddings\n",
    "\n",
    "\n",
    "token_embeddings = embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "token_embeddings.shape\n",
    "\n",
    "vocabulary = tokenizer.get_vocab()\n",
    "sorted_vocabulary = sorted(\n",
    "    vocabulary.items(),\n",
    "    key=lambda x: x[1],  # uses the value of the dictionary entry\n",
    ")\n",
    "sorted_tokens = [token for token, _ in sorted_vocabulary]\n",
    "random.choices(sorted_tokens, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, metric=\"cosine\", random_state=42)\n",
    "tsne_embeddings_2d = tsne.fit_transform(token_embeddings)\n",
    "tsne_embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_colors = []\n",
    "for token in sorted_tokens:\n",
    "    if token[0] == \"[\" and token[-1] == \"]\":\n",
    "        token_colors.append(\"red\")\n",
    "    elif token.startswith(\"##\"):\n",
    "        token_colors.append(\"blue\")\n",
    "    else:\n",
    "        token_colors.append(\"green\")\n",
    "\n",
    "\n",
    "scatter = go.Scattergl(\n",
    "    x=tsne_embeddings_2d[:, 0],\n",
    "    y=tsne_embeddings_2d[:, 1],\n",
    "    text=sorted_tokens,\n",
    "    marker=dict(color=token_colors, size=3),\n",
    "    mode=\"markers\",\n",
    "    name=\"Token embeddings\",\n",
    ")\n",
    "\n",
    "fig = go.FigureWidget(\n",
    "    data=[scatter],\n",
    "    layout=dict(\n",
    "        width=600,\n",
    "        height=900,\n",
    "        margin=dict(l=0, r=0),\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
