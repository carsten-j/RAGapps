{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG application for Q&A with PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG is short for Retrieval-Augmented Generation and the term was coined in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf).\n",
    "\n",
    "The architecture of the proposed model was:\n",
    "\n",
    "<img src=\"images/rag-architecture.png\" width=\"800\">\n",
    "\n",
    "For an overview of RAG models check the paper [Retrieval-Augmented Generation for Large\n",
    "Language Models: A Survey](https://arxiv.org/abs/2312.10997).\n",
    "\n",
    "Martin Fowler recently published a blog post [Emerging Patterns in Building GenAI Products](https://martinfowler.com/articles/gen-ai-patterns/) that contains a lot of valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT DRAWING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps required to build a RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __PDF Extraction and Preprocessing__  \n",
    "\n",
    "   • Extract Text: Use libraries like PyPDFLoader, PyPDF2, pdfplumber, or similar tools to extract the text content from the PDF file.  \n",
    "   • Clean and Preprocess: Remove unnecessary formatting, fix encoding issues, and possibly normalize the text (e.g., lowercasing, punctuation handling).  \n",
    "   • Document Segmentation: Depending on your PDF’s structure, you might want to segment it by chapters, sections, or pages if needed.\n",
    "\n",
    "2. __Chunking the Documents__  \n",
    "\n",
    "   • Define Chunk Size: Split the extracted text into manageable chunks (e.g., paragraphs or fixed-size windows) so that each piece can be meaningfully processed.  \n",
    "   • Overlap Chunks: Optionally use overlapping windows to ensure smooth context transitions between chunks, which helps when a concept spans multiple chunks.\n",
    "\n",
    "3. __Creating Embeddings for the Text Chunks__  \n",
    "\n",
    "   • Choose an Embedding Model: Use a state-of-the-art embedding model (e.g., OpenAI’s embedding APIs, Sentence Transformers, etc.) that maps text chunks to high-dimensional vectors.  \n",
    "   • Generate Embeddings: Iterate over the chunks and compute their embeddings. This turns each text snippet into a vector which captures semantic meaning.\n",
    "\n",
    "4. __Building a Vector Index__\n",
    "\n",
    "   • Select a Vector Store: Use libraries such as Qdrant, Chroma, Faiss or Pinecone to store and index your embeddings.  \n",
    "   • Insert Embeddings: Store each vector along with metadata (like the chunk text, source page, or document section) for quick retrieval later on.\n",
    "\n",
    "5. __Setting Up the Retrieval Mechanism__  \n",
    "\n",
    "   • Query Embedding: When a user submits a question, embed the question using the same embedding model.  \n",
    "   • Similarity Search: Query the vector index to retrieve the top-n most similar text chunks based on the question’s embedding.  \n",
    "\n",
    "6. __Constructing the RAG Pipeline__  \n",
    "\n",
    "   • Context Combination: Concatenate the retrieved chunks into a context prompt or pass them as additional inputs to the LLM.  \n",
    "   • Prompt Engineering: Craft a prompt that combines the user’s question with the retrieved context. Ensure the prompt instructs the LLM to use the provided evidence to answer the query.  \n",
    "   • LLM Query: Use an LLM (like GPT-4) to generate the final answer based on both the question and the supporting context from the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is based on [LangChain](https://www.langchain.com/) which is a composable framework to build with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Let's explore a couple of LLM's before we build the RAG application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the famous \"Attention Is All You Need\" paper as source document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the paper\n",
    "# !wget https://arxiv.org/pdf/1706.03762\n",
    "# !mv 1706.03762 PDFs/attention.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"I am reading the 'Attention is all you need' paper! \"\n",
    "\n",
    "# Text\n",
    "query = prefix + \"How many GPU's were the models trained on?\"\n",
    "# Answer: 8 GPUs\n",
    "\n",
    "# Table\n",
    "# query = (\n",
    "#     prefix\n",
    "#     + \"What is the BLEU score of the MoE model for English-to-German translation?\"\n",
    "# )\n",
    "# Answer: 26.03\n",
    "\n",
    "# Image\n",
    "# query = prefix + \"What does the sentence in figure 5 say?\"\n",
    "# Answer:\n",
    "# The Law will never be perfect, but its application should be just.\n",
    "# This is what we are missing, in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original “Attention Is All You Need” paper (Vaswani et al., 2017) does not include a Mixture-of-Experts (MoE) variant. In that paper, the authors report BLEU scores for what they call “Transformer (base)” and “Transformer (big)” models on WMT 2014 English→German and English→French, but there is no MoE experiment.\n",
      "\n",
      "If you are looking for a Mixture-of-Experts approach by some of the same researchers, you may be thinking of separate follow-up work on MoE layers (e.g. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer” by Shazeer et al., 2017) or subsequent papers like Switch Transformers and GShard. However, those MoE results do not appear in the original “Attention Is All You Need” paper.\n"
     ]
    }
   ],
   "source": [
    "llm_openai = ChatOpenAI(model=\"o1\")\n",
    "\n",
    "openai = prompt | llm_openai\n",
    "\n",
    "response_openai = openai.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=query,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_openai.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out the BLEU score for the MoE model in the English-to-German translation task from the 'Attention is All You Need' paper. I remember that BLEU is a common metric used to evaluate machine translation models, but I'm not exactly sure how it's calculated or what factors influence its value.\n",
      "\n",
      "First, I think about what BLEU measures. It stands for Bilingual Evaluation Underlying Small Trees, and it's based on the number of exact matches between the predicted and actual sequences. So, higher scores mean better performance because there are more exact matches. But wait, isn't that not always the case? Maybe sometimes models can have multiple near-matches without exact matches, which could affect the score.\n",
      "\n",
      "I also recall that BLEU is sensitive to the diversity of the outputs. If a model produces many similar but incorrect translations, it might still perform well because there are enough exact matches elsewhere. But if all the outputs are very similar and miss some correct ones, the score might drop.\n",
      "\n",
      "Now, thinking about the MoE model in this context. The paper introduced Model-Aware Attention (MoE), which is a way to improve attention mechanisms by considering multiple models for different parts of the input or output. I'm not sure how that affects the BLEU score directly, but it's likely that the MoE architecture might help with better translation quality.\n",
      "\n",
      "I wonder if there are any studies comparing MoE models against other architectures like Transformer or RNNs on this task. Maybe some experiments have been done where they used MoE and compared their BLEU scores to other models. If I could find a paper or dataset that does this, it might give me an idea of what the score is.\n",
      "\n",
      "I also think about the training process. Does the model get enough data? How long was the training run? The quality of the data and the computational resources used would definitely impact the BLEU score. More diverse training data and longer runs could lead to better models, which might have higher scores.\n",
      "\n",
      "Another factor is the complexity of the task. English-to-German translation is a complex language pair with many nuances. If the model is too simple or not capturing enough context, it might not perform well, leading to lower scores. On the other hand, if it's too complex, maybe it overfits and doesn't generalize well.\n",
      "\n",
      "I also recall that BLEU can be influenced by the evaluation set. How was the dataset split? Were there specific metrics used for translation quality? The way the data is processed could affect how well the model performs in terms of exact matches versus near-matches.\n",
      "\n",
      "Maybe I should look into any benchmarks or studies that have evaluated MoE models on this task. Perhaps there's a leaderboard or a paper that reports the performance of MoE models against other architectures. If such information exists, it would be valuable to know the current state of the art in terms of BLEU scores for this specific translation task.\n",
      "\n",
      "I also think about potential improvements. The MoE architecture might introduce some biases or inefficiencies that could affect the score. Maybe there's a way to optimize the MoE components to improve performance without compromising on exact matches.\n",
      "\n",
      "In summary, while I don't have the exact BLEU score for the MoE model in this task, factors like data quality, architectural complexity, training resources, and evaluation criteria all play roles. To get an accurate answer, I would need to look into specific studies or experiments that compare MoE models against other approaches on English-to-German translation using thebleu metric.\n",
      "</think>\n",
      "\n",
      "The BLEU score for the MoE model in the English-to-German translation task is influenced by several factors:\n",
      "\n",
      "1. **Data Quality and Quantity**: The quality and diversity of the training data significantly impact the score, as higher diversity can lead to more exact matches.\n",
      "\n",
      "2. **Architectural Complexity**: The MoE architecture may offer improvements over simpler models like Transformers or RNNs in capturing context and nuances, potentially leading to better performance.\n",
      "\n",
      "3. **Training Resources and Computational Constraints**: More diverse training data and longer computational resources can enhance model performance, which might result in higher scores.\n",
      "\n",
      "4. **Task Complexity**: The complexity of the language pair (English-German) requires models that capture multiple contextual aspects, which could improve translation quality and score.\n",
      "\n",
      "5. **Evaluation Metrics**: The specific evaluation metrics used, such as BLEU, measure exact matches, but other factors like diversity can influence performance.\n",
      "\n",
      "6. **Architectural Biases**: The MoE architecture might introduce biases, potentially affecting the score by introducing inefficiencies in model components.\n",
      "\n",
      "To determine the current state of the art, it is advisable to consult specific studies or experiments that compare MoE models against other architectures on this task using thebleu metric.\n"
     ]
    }
   ],
   "source": [
    "deepseek = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", temperature=0, base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "chain_deepseek = prompt | deepseek\n",
    "\n",
    "response_deepseek = chain_deepseek.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=query,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_deepseek.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the knowledge base\n",
    "### Download source document\n",
    "#### Step 1 Extract text from \n",
    "\n",
    "Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./PDFs/attention.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `document` corresponds to one page in the PDF file. Let us explore the content of the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz \n"
     ]
    }
   ],
   "source": [
    "print(f\"{documents[0].page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25',\n",
      " 'creator': 'LaTeX with hyperref',\n",
      " 'creationdate': '2024-04-10T21:11:43+00:00',\n",
      " 'author': '',\n",
      " 'keywords': '',\n",
      " 'moddate': '2024-04-10T21:11:43+00:00',\n",
      " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live '\n",
      "                    '2023) kpathsea version 6.3.5',\n",
      " 'subject': '',\n",
      " 'title': '',\n",
      " 'trapped': '/False',\n",
      " 'source': './PDFs/attention.pdf',\n",
      " 'total_pages': 15,\n",
      " 'page': 0,\n",
      " 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting text can be anything from as easy as this simple example or as complex as you want it to be. It is much harder if you want to preserve metadata as chapters, sections etc. Extracting text from tables is not easy and it is even harder with figures and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Chuncking the document\n",
    "#### Text splitter\n",
    "\n",
    "Split each page into smaller chunks. \n",
    "\n",
    "Set `add_start_index=True` to preserved meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './PDFs/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1610}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "### Create Embeddings\n",
    "\n",
    "We load an embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/qdrant/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 768\n",
      "\n",
      "first 5 elements in embedding vector: \n",
      "[0.00345193431712687, 0.01597711443901062, -0.013028663583099842, 0.0009539231541566551, -0.051165636628866196]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = embeddings.embed_query(chunks[0].page_content)\n",
    "\n",
    "embedding_vector_length = len(embedding_vector)\n",
    "\n",
    "print(f\"Generated vectors of length {embedding_vector_length}\\n\")\n",
    "print(f\"first 5 elements in embedding vector: \\n{embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "### Create Vector Index\n",
    "\n",
    "Here we will be using the vector database [Qdrant](https://qdrant.tech/) running locally in Docker.\n",
    "\n",
    "Note we set the config value for vector lenght to be the same as produced by the embedding model, i.e. 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"attention\"\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "collection_exists = client.collection_exists(collection_name=collection_name)\n",
    "\n",
    "if not collection_exists:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=embedding_vector_length, distance=Distance.COSINE\n",
    "        ),\n",
    "    )\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add embeddings to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not collection_exists:\n",
    "    print(\"Adding documents to the collection\")\n",
    "    ids = vector_store.add_documents(documents=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "### Search vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop = 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query)\n",
    "\n",
    "print(results[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "### Constructing the RAG pipeline\n",
    "\n",
    "Setup Q&A with LLM using vector database as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/qdrant/.venv/lib/python3.12/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out the BLEU score for the MoE model in English-to-German translation. The user mentioned that they're reading the 'Attention is all you need' paper and specifically wants to know about the MoE model's BLEU score.\n",
      "\n",
      "Looking at the context provided, it seems like there are several sections discussing different models, including Transformers and their configurations. In one part, under \"6 Results,\" there's a section titled \"Machine Translation\" where they talk about the big Transformer model. It mentions that on the English-to-German task, this model achieved a BLEU score of 28.4, which is higher than all previously published models and ensembles.\n",
      "\n",
      "Additionally, it notes that for the base model (which I assume refers to the configuration used in the experiments), they use a dropout rate Pdrop = 0.1 and label smoothing ϵls = 0.1. This might be relevant if there's another MoE variant mentioned later on, but based on the context given, it seems like the main focus is on the big Transformer model.\n",
      "\n",
      "So, putting this together, I think the key information here is that the base model (big Transformer) achieved a BLEU score of 28.4. The other details about dropout and label smoothing are probably for different configurations or models not mentioned in the provided context.\n",
      "</think>\n",
      "\n",
      "The base MoE model (big Transformer) achieved ableu score of 28.4 on the English-to-German translation task, outperforming all previously published models and ensembles at a fraction of the training cost.\n"
     ]
    }
   ],
   "source": [
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | deepseek\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "Simple RAG applications are \"just\" advanced prompt engineering!\n",
    "\n",
    "Be aware of the size of the context window when injecting context from the vector database.\n",
    "\n",
    "* Multimodal RAG\n",
    "* More advanced techniques\n",
    "  * Hybrid search\n",
    "  * Re-ranking\n",
    "  * Metadata filtering\n",
    "* CAG\n",
    "* \n",
    "\n",
    "## What did we not cover?\n",
    "\n",
    "* Evaluation of RAG models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
