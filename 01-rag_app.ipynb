{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG application for Q&A with PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG is short for Retrieval-Augmented Generation and the term was coined in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf).\n",
    "\n",
    "The architecture of the proposed model was:\n",
    "\n",
    "<img src=\"images/rag-architecture.png\" width=\"1200\">\n",
    "\n",
    "For an overview of RAG models check the paper [Retrieval-Augmented Generation for Large\n",
    "Language Models: A Survey](https://arxiv.org/abs/2312.10997).\n",
    "\n",
    "Martin Fowler recently published a blog post [Emerging Patterns in Building GenAI Products](https://martinfowler.com/articles/gen-ai-patterns/) that contains a lot of valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model overview\n",
    "\n",
    "<img src=\"images/overview.jpg\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world use case of RAG\n",
    "\n",
    "<img src=\"images/kol.jpg\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps required to build a RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __PDF Extraction and Preprocessing__  \n",
    "\n",
    "   • Extract Text: Use libraries like PyPDFLoader, PyPDF2, pdfplumber, or similar tools to extract the text content from the PDF file.  \n",
    "   • Clean and Preprocess: Remove unnecessary formatting, fix encoding issues, and possibly normalize the text (e.g., lowercasing, punctuation handling).  \n",
    "   • Document Segmentation: Depending on your PDF’s structure, you might want to segment it by chapters, sections, or pages if needed.\n",
    "\n",
    "2. __Chunking the Documents__  \n",
    "\n",
    "   • Define Chunk Size: Split the extracted text into manageable chunks (e.g., paragraphs or fixed-size windows) so that each piece can be meaningfully processed.  \n",
    "   • Overlap Chunks: WHY?\n",
    "\n",
    "3. __Creating Embeddings for the Text Chunks__  \n",
    "\n",
    "   • Choose an Embedding Model: Use a state-of-the-art embedding model (e.g., OpenAI’s embedding APIs, Sentence Transformers, etc.) that maps text chunks to high-dimensional vectors.  \n",
    "   • Generate Embeddings: Iterate over the chunks and compute their embeddings. This turns each text snippet into a vector which captures semantic meaning.\n",
    "\n",
    "4. __Building a Vector Index__\n",
    "\n",
    "   • Select a Vector Store: Use libraries such as Qdrant, Chroma, Faiss or Pinecone to store and index your embeddings.  \n",
    "   • Insert Embeddings: Store each vector along with metadata (like the chunk text, source page, or document section) for quick retrieval later on.\n",
    "\n",
    "5. __Setting Up the Retrieval Mechanism__  \n",
    "\n",
    "   • Query Embedding: When a user submits a question, embed the question using the same embedding model.  \n",
    "   • Similarity Search: Query the vector index to retrieve the top-n most similar text chunks based on the question’s embedding.  \n",
    "\n",
    "6. __Constructing the RAG Pipeline__  \n",
    "\n",
    "   • Context Combination: Concatenate the retrieved chunks into a context prompt or pass them as additional inputs to the LLM.  \n",
    "   • Prompt Engineering: Craft a prompt that combines the user’s question with the retrieved context. Ensure the prompt instructs the LLM to use the provided evidence to answer the query.  \n",
    "   • LLM Query: Use an LLM (like GPT-4) to generate the final answer based on both the question and the supporting context from the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is based on [LangChain](https://www.langchain.com/) which is a composable framework to build with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Let's explore a couple of LLM's before we build the RAG application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the famous \"Attention Is All You Need\" paper as source document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the paper\n",
    "# !wget https://arxiv.org/pdf/1706.03762\n",
    "# !mv 1706.03762 PDFs/attention.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"I am reading the 'Attention is all you need' paper! \"\n",
    "\n",
    "# Text\n",
    "query = prefix + \"How many GPU's were the models trained on?\"\n",
    "# Answer: 8 GPUs - page 7\n",
    "\n",
    "# Table\n",
    "# query = (\n",
    "#     prefix\n",
    "#     + \"What is the BLEU score of the MoE model for English-to-German translation?\"\n",
    "# )\n",
    "# Answer: 26.03 - page 8\n",
    "\n",
    "# Image\n",
    "# query = prefix + \"What does the sentence in figure 5 say?\"\n",
    "# Answer: - page 15\n",
    "# The Law will never be perfect, but its application should be just.\n",
    "# This is what we are missing, in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the original paper (“Attention Is All You Need,” Vaswani et al., 2017), the authors trained their Transformer models on 8 NVIDIA P100 GPUs. Specifically, in Section 5.3 (Training), they mention that the “base” Transformer model trains for about 12 hours on 8 P100 GPUs, while the larger model (“Transformer big”) takes around 3.5 days on the same setup.\n"
     ]
    }
   ],
   "source": [
    "openai = ChatOpenAI(model=\"o1\")\n",
    "\n",
    "openai_chain = prompt | openai\n",
    "\n",
    "response_openai = openai_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=query,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_openai.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how many GPUs the models from the \"Attention is All You Need\" paper were trained on. I remember that this paper introduced a transformer-based model for machine translation, and it was pretty influential. But I'm not exactly sure about the specifics of hardware requirements.\n",
      "\n",
      "First, I think about what the original setup might have been. The authors probably used some kind of cluster or supercomputer because training large models like this would require a lot of computational power. I recall that Google did some work with GPUs back then, but maybe they were using more than one GPU per machine.\n",
      "\n",
      "I also remember that each machine in the cluster had multiple GPUs. For example, if there were 8 GPUs on a single machine, that's a common setup for multi-GPU training. So, if each of the 16 machines had 8 GPUs, that would be 128 GPUs in total. That makes sense because having more GPUs per machine allows for parallel training, which is essential for handling the large datasets and model sizes.\n",
      "\n",
      "Wait, but I'm not entirely sure about the exact number. Maybe it was different back then? I think Google used a lot of powerful hardware back when this paper came out. They probably had clusters with multiple machines each equipped with several GPUs. So, 16 machines times 8 GPUs each would give us 128 GPUs in total.\n",
      "\n",
      "I also wonder about the specific hardware configuration. Were they using NVIDIA GPUs? I believe so because that's a common choice for such tasks. The combination of multiple GPUs per machine and the number of machines would significantly impact the training time, especially since transformers are computationally intensive.\n",
      "\n",
      "So putting it all together, the models were trained on 128 GPUs. That seems to align with what I know about the original setup back then. It's important because having more GPUs allows for faster training by utilizing parallel computing resources.\n",
      "</think>\n",
      "\n",
      "The models from the \"Attention is All You Need\" paper were trained using a total of 128 GPUs. This configuration involved 16 machines, each equipped with multiple GPUs (likely 8 per machine), enabling parallel training and significantly reducing the overall computational requirements for large-scale transformer-based models.\n"
     ]
    }
   ],
   "source": [
    "deepseek = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", temperature=0, base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "chain_deepseek = prompt | deepseek\n",
    "\n",
    "response_deepseek = chain_deepseek.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=query,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_deepseek.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the knowledge base\n",
    "### Download source document\n",
    "#### Step 1 Extract text from \n",
    "\n",
    "Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./PDFs/attention.pdf\")\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `document` corresponds to one page in the PDF file. Let us explore the content of the first document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz \n"
     ]
    }
   ],
   "source": [
    "print(f\"{documents[0].page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25',\n",
      " 'creator': 'LaTeX with hyperref',\n",
      " 'creationdate': '2024-04-10T21:11:43+00:00',\n",
      " 'author': '',\n",
      " 'keywords': '',\n",
      " 'moddate': '2024-04-10T21:11:43+00:00',\n",
      " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live '\n",
      "                    '2023) kpathsea version 6.3.5',\n",
      " 'subject': '',\n",
      " 'title': '',\n",
      " 'trapped': '/False',\n",
      " 'source': './PDFs/attention.pdf',\n",
      " 'total_pages': 15,\n",
      " 'page': 0,\n",
      " 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting text can be anything from as easy as this simple example or as complex as you want it to be. It is much harder if you want to preserve metadata as chapters, sections etc. Extracting text from tables is not easy and it is even harder with figures and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "### Chuncking the document\n",
    "#### Text splitter\n",
    "\n",
    "Split each page into smaller chunks. \n",
    "\n",
    "Set `add_start_index=True` to preserved meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './PDFs/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'start_index': 1610}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "### Create Embeddings\n",
    "\n",
    "#### Embeddings visualized\n",
    "\n",
    "<img src=\"images/embedding_vector_space.png\" width=\"1200\">\n",
    "\n",
    "We load an embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/RAGapps/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 768\n",
      "\n",
      "first 5 elements in embedding vector: \n",
      "[0.00345193431712687, 0.01597711443901062, -0.013028663583099842, 0.0009539231541566551, -0.051165636628866196]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = embeddings.embed_query(chunks[0].page_content)\n",
    "\n",
    "embedding_vector_length = len(embedding_vector)\n",
    "\n",
    "print(f\"Generated vectors of length {embedding_vector_length}\\n\")\n",
    "print(f\"first 5 elements in embedding vector: \\n{embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "### Create Vector Index\n",
    "\n",
    "Here we will be using the vector database [Qdrant](https://qdrant.tech/) running locally in Docker.\n",
    "\n",
    "Note we set the config value for vector lenght to be the same as produced by the embedding model, i.e. 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"attention\"\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "collection_exists = client.collection_exists(collection_name=collection_name)\n",
    "\n",
    "if not collection_exists:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=embedding_vector_length, distance=Distance.COSINE\n",
    "        ),\n",
    "    )\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add embeddings to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not collection_exists:\n",
    "    print(\"Adding documents to the collection\")\n",
    "    ids = vector_store.add_documents(documents=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "### Search vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate = d−0.5\n",
      "model · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps = 4000.\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query)\n",
    "\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "### Constructing the RAG pipeline\n",
    "\n",
    "Setup Q&A with LLM using vector database as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carstenj/dev/RAGapps/.venv/lib/python3.12/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how many GPUs were used in the 'Attention is all you need' paper. The user provided a context where it says they trained on one machine with 8 NVIDIA P100 GPUs. That means each GPU was part of that single machine setup. So, even though there are eight GPUs, they're all working together on the same computer. I don't think the number is asking for how many were used in total but rather how many were available or part of the training environment. So the answer should be 8 P100 GPUs.\n",
      "</think>\n",
      "\n",
      "The paper was trained on one machine with 8 NVIDIA P100 GPUs, meaning all eight were part of the same setup.\n"
     ]
    }
   ],
   "source": [
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | deepseek\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "Simple RAG applications are \"just\" advanced prompt engineering!\n",
    "\n",
    "Be aware of the size of the context window when injecting context from the vector database.\n",
    "\n",
    "* Multimodal RAG\n",
    "* More advanced techniques\n",
    "  * Hybrid search\n",
    "  * Re-ranking\n",
    "  * Metadata filtering\n",
    "* CAG\n",
    "\n",
    "## What did we not cover?\n",
    "\n",
    "* Evaluation of RAG models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
