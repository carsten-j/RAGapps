{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@anubhavm48/practical-introduction-to-llm-tools-in-lang-chain-bb6b8b42d8b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "\n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Target the main article content for LangGraph documentation\n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "\n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "\n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def load_langgraph_docs():\n",
    "    \"\"\"\n",
    "    Load LangGraph documentation from the official website.\n",
    "\n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the LangGraph website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading LangGraph documentation...\")\n",
    "\n",
    "    # Load the documentation\n",
    "    urls = [\n",
    "        \"https://langchain-ai.github.io/langgraph/concepts/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/how-tos/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "        \"https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\",\n",
    "    ]\n",
    "\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=5,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from LangGraph documentation.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i + 1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "\n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "\n",
    "    return docs, tokens_per_doc\n",
    "\n",
    "\n",
    "def save_llms_full(documents):\n",
    "    \"\"\"Save the documents to a file\"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"llms_full.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get(\"source\", \"Unknown URL\")\n",
    "\n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i + 1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n",
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "\n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "\n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "\n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000, chunk_overlap=500\n",
    "    )\n",
    "\n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "\n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "\n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "\n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "\n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "\n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "\n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd() + \"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "\n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LangGraph documentation...\n",
      "Loaded 123 documents from LangGraph documentation.\n",
      "\n",
      "Loaded URLs:\n",
      "1. https://langchain-ai.github.io/langgraph/concepts/\n",
      "2. https://langchain-ai.github.io/langgraph/concepts/functional_api/\n",
      "3. https://langchain-ai.github.io/langgraph/concepts/time-travel/\n",
      "4. https://langchain-ai.github.io/langgraph/concepts/streaming/\n",
      "5. https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/\n",
      "6. https://langchain-ai.github.io/langgraph/concepts/multi_agent/\n",
      "7. https://langchain-ai.github.io/langgraph/concepts/breakpoints\n",
      "8. https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\n",
      "9. https://langchain-ai.github.io/langgraph/concepts/faq/\n",
      "10. https://langchain-ai.github.io/langgraph/concepts/low_level/\n",
      "11. https://langchain-ai.github.io/langgraph/concepts/pregel/\n",
      "12. https://langchain-ai.github.io/langgraph/concepts/durable_execution/\n",
      "13. https://langchain-ai.github.io/langgraph/concepts/memory/\n",
      "14. https://langchain-ai.github.io/langgraph/concepts/persistence/\n",
      "15. https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "16. https://langchain-ai.github.io/langgraph/concepts/self_hosted/\n",
      "17. https://langchain-ai.github.io/langgraph/concepts/bring_your_own_cloud/\n",
      "18. https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/\n",
      "19. https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/&\n",
      "20. https://langchain-ai.github.io/langgraph/concepts/langgraph_server/\n",
      "21. https://langchain-ai.github.io/langgraph/concepts/deployment_options/\n",
      "22. https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\n",
      "23. https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/\n",
      "24. https://langchain-ai.github.io/langgraph/concepts/assistants/\n",
      "25. https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/\n",
      "26. https://langchain-ai.github.io/langgraph/concepts/sdk/\n",
      "27. https://langchain-ai.github.io/langgraph/concepts/auth/\n",
      "28. https://langchain-ai.github.io/langgraph/concepts/double_texting/\n",
      "29. https://langchain-ai.github.io/langgraph/concepts/application_structure/\n",
      "30. https://langchain-ai.github.io/langgraph/concepts/self_hosted/&\n",
      "31. https://langchain-ai.github.io/langgraph/concepts/plans/\n",
      "32. https://langchain-ai.github.io/langgraph/concepts/plans/&\n",
      "33. https://langchain-ai.github.io/langgraph/concepts/template_applications/\n",
      "34. https://langchain-ai.github.io/langgraph/concepts/platform_architecture/\n",
      "35. https://langchain-ai.github.io/langgraph/concepts/breakpoints/\n",
      "36. https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/\n",
      "37. https://langchain-ai.github.io/langgraph/how-tos/\n",
      "38. https://langchain-ai.github.io/langgraph/how-tos/pass-config-to-tools/\n",
      "39. https://langchain-ai.github.io/langgraph/how-tos/many-tools/\n",
      "40. https://langchain-ai.github.io/langgraph/how-tos/tool-calling-errors/\n",
      "41. https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/\n",
      "42. https://langchain-ai.github.io/langgraph/how-tos/update-state-from-tools/\n",
      "43. https://langchain-ai.github.io/langgraph/how-tos/tool-calling/\n",
      "44. https://langchain-ai.github.io/langgraph/how-tos/pass-run-time-values-to-tools/\n",
      "45. https://langchain-ai.github.io/langgraph/how-tos/subgraph/\n",
      "46. https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/\n",
      "47. https://langchain-ai.github.io/langgraph/how-tos/subgraph-transform-state/\n",
      "48. https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/\n",
      "49. https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/\n",
      "50. https://langchain-ai.github.io/langgraph/how-tos/autogen-integration-functional/\n",
      "51. https://langchain-ai.github.io/langgraph/how-tos/async/\n",
      "52. https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/\n",
      "53. https://langchain-ai.github.io/langgraph/how-tos/autogen-integration/\n",
      "54. https://langchain-ai.github.io/langgraph/how-tos/run-id-langsmith/\n",
      "55. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "56. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-system-prompt/\n",
      "57. https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional/\n",
      "58. https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/\n",
      "59. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-structured-output/\n",
      "60. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/\n",
      "61. https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-hitl/\n",
      "62. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/\n",
      "63. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo-functional/\n",
      "64. https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/\n",
      "65. https://langchain-ai.github.io/langgraph/how-tos/command\n",
      "66. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network-functional/\n",
      "67. https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network/\n",
      "68. https://langchain-ai.github.io/langgraph/how-tos/state-model/\n",
      "69. https://langchain-ai.github.io/langgraph/how-tos/input_output_schema/\n",
      "70. https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs\n",
      "71. https://langchain-ai.github.io/langgraph/how-tos/autogen-langgraph-platform/\n",
      "72. https://langchain-ai.github.io/langgraph/how-tos/subgraph-persistence/\n",
      "73. https://langchain-ai.github.io/langgraph/how-tos/persistence/\n",
      "74. https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/\n",
      "75. https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence/\n",
      "76. https://langchain-ai.github.io/langgraph/how-tos/persistence-functional/\n",
      "77. https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence-functional/\n",
      "78. https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/\n",
      "79. https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/\n",
      "80. https://langchain-ai.github.io/langgraph/how-tos/return-when-recursion-limit-hits/\n",
      "81. https://langchain-ai.github.io/langgraph/how-tos/command/\n",
      "82. https://langchain-ai.github.io/langgraph/how-tos/node-retries/\n",
      "83. https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\n",
      "84. https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/\n",
      "85. https://langchain-ai.github.io/langgraph/how-tos/configuration/\n",
      "86. https://langchain-ai.github.io/langgraph/how-tos/subgraph\n",
      "87. https://langchain-ai.github.io/langgraph/how-tos/auth/custom_auth/\n",
      "88. https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/\n",
      "89. https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/\n",
      "90. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/\n",
      "91. https://langchain-ai.github.io/langgraph/how-tos/review-tool-calls-functional/\n",
      "92. https://langchain-ai.github.io/langgraph/how-tos/streaming/\n",
      "93. https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/\n",
      "94. https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/\n",
      "95. https://langchain-ai.github.io/langgraph/how-tos/streaming-specific-nodes/\n",
      "96. https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/\n",
      "97. https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens\n",
      "98. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/\n",
      "99. https://langchain-ai.github.io/langgraph/how-tos/wait-user-input-functional/\n",
      "100. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/\n",
      "101. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/dynamic_breakpoints/\n",
      "102. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/\n",
      "103. https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/\n",
      "104. https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional\n",
      "105. https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/\n",
      "106. https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/\n",
      "107. https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages\n",
      "108. https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/\n",
      "109. https://langchain-ai.github.io/langgraph/how-tos/branching/\n",
      "110. https://langchain-ai.github.io/langgraph/how-tos/state-reducers\n",
      "111. https://langchain-ai.github.io/langgraph/how-tos/state-reducers/\n",
      "112. https://langchain-ai.github.io/langgraph/how-tos/visualization\n",
      "113. https://langchain-ai.github.io/langgraph/how-tos/state-model\n",
      "114. https://langchain-ai.github.io/langgraph/how-tos/visualization/\n",
      "115. https://langchain-ai.github.io/langgraph/how-tos/sequence/\n",
      "116. https://langchain-ai.github.io/langgraph/how-tos/http/custom_middleware/\n",
      "117. https://langchain-ai.github.io/langgraph/how-tos/http/custom_routes/\n",
      "118. https://langchain-ai.github.io/langgraph/how-tos/http/custom_lifespan/\n",
      "119. https://langchain-ai.github.io/langgraph/how-tos/auth/openapi_security/\n",
      "120. https://langchain-ai.github.io/langgraph/how-tos/local-studio/\n",
      "121. https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
      "122. https://langchain-ai.github.io/langgraph/tutorials/introduction/\n",
      "123. https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/\n",
      "Total tokens in loaded documents: 302642\n",
      "Documents concatenated into llms_full.txt\n",
      "Splitting documents...\n",
      "Created 136 chunks from documents.\n",
      "Total tokens in split documents: 305740\n",
      "Creating SKLearnVectorStore...\n",
      "SKLearnVectorStore created successfully.\n",
      "SKLearnVectorStore was persisted to /Users/carsten/Dev/RAGapps/sklearn_vectorstore.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_langgraph_docs()\n",
    "\n",
    "# Save the documents to a file\n",
    "save_llms_full(documents)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = split_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n",
      "https://langchain-ai.github.io/langgraph/concepts/high_level/\n",
      "Why LangGraph?¶\n",
      "LLM applications¶\n",
      "LLMs make it possible to embed intelligence into a new class of applications. There are many patterns for building applications that use LLMs. Workflows have scaffolding of predefined code paths around LLM calls. LLMs can direct the control flow through these predefined code paths, which some consider to be an \"agentic system\". In other cases, it's possible to remove this scaffolding, creating autonomous agents that can plan, take actions via tool calls, and dir\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/concepts/faq/\n",
      "FAQ¶\n",
      "Common questions and their answers!\n",
      "Do I need to use LangChain to use LangGraph? What’s the difference?¶\n",
      "No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\n",
      "How is LangGraph different from other agent frameworks?¶\n",
      "Other agentic frameworks can work for simple, generic tas\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/\n",
      "LangGraph Platform¶\n",
      "Overview¶\n",
      "LangGraph Platform is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework.\n",
      "The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications:\n",
      "\n",
      "LangGraph Server: The server defines an opinionated API and architecture that incorporates best practices for deploying agentic applications, allowing you to focus\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Get relevant documents for the query\n",
    "query = \"What is LangGraph?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata[\"source\"])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def langgraph_query_tool(query: str):\n",
    "    \"\"\"\n",
    "    Query the LangGraph documentation using a retriever.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search the documentation with\n",
    "\n",
    "    Returns:\n",
    "        str: A str of the retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = SKLearnVectorStore(\n",
    "        embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "        persist_path=os.getcwd() + \"/sklearn_vectorstore.parquet\",\n",
    "        serializer=\"parquet\",\n",
    "    ).as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "    formatted_context = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"==DOCUMENT {i + 1}==\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(relevant_docs)\n",
    "        ]\n",
    "    )\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  langgraph_query_tool (call_BM4vBjFgbIQiTZ0Pz2j8Iccq)\n",
      " Call ID: call_BM4vBjFgbIQiTZ0Pz2j8Iccq\n",
      "  Args:\n",
      "    query: What is LangGraph?\n"
     ]
    }
   ],
   "source": [
    "llm = llm = ChatOpenAI(\n",
    "    model=\"o1\"\n",
    "    # other params...\n",
    ")\n",
    "augmented_llm = llm.bind_tools([langgraph_query_tool], strict=True)\n",
    "\n",
    "instructions = \"\"\"You are a helpful assistant that can answer questions about the LangGraph documentation. \n",
    "Use the langgraph_query_tool for any questions about the documentation.\n",
    "If you don't know the answer, say \"I don't know.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": \"What is LangGraph?\"},\n",
    "]\n",
    "\n",
    "message = augmented_llm.invoke(messages)\n",
    "message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mZYzxj9Xr8AMqBZod5WhJGRI', 'function': {'arguments': '{\"query\":\"What is LangGraph?\"}', 'name': 'langgraph_query_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 160, 'prompt_tokens': 125, 'total_tokens': 285, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-2024-12-17', 'system_fingerprint': 'fp_6a8075f534', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5b7f24b7-549b-4a40-bb5f-3c58baf9b531-0', tool_calls=[{'name': 'langgraph_query_tool', 'args': {'query': 'What is LangGraph?'}, 'id': 'call_mZYzxj9Xr8AMqBZod5WhJGRI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 125, 'output_tokens': 160, 'total_tokens': 285, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIMessage(\n",
    "    content=\"\",\n",
    "    additional_kwargs={\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": \"call_o9udf3EVOWiV4Iupktpbpofk\",\n",
    "                \"function\": {\n",
    "                    \"arguments\": '{\"location\":\"San Francisco, CA\"}',\n",
    "                    \"name\": \"GetWeather\",\n",
    "                },\n",
    "                \"type\": \"function\",\n",
    "            }\n",
    "        ],\n",
    "        \"refusal\": None,\n",
    "    },\n",
    "    response_metadata={\n",
    "        \"token_usage\": {\n",
    "            \"completion_tokens\": 17,\n",
    "            \"prompt_tokens\": 68,\n",
    "            \"total_tokens\": 85,\n",
    "        },\n",
    "        \"model_name\": \"gpt-4o-2024-05-13\",\n",
    "        \"system_fingerprint\": \"fp_3aa7262c27\",\n",
    "        \"finish_reason\": \"tool_calls\",\n",
    "        \"logprobs\": None,\n",
    "    },\n",
    "    id=\"run-1617c9b2-dda5-4120-996b-0333ed5992e2-0\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"name\": \"GetWeather\",\n",
    "            \"args\": {\"location\": \"San Francisco, CA\"},\n",
    "            \"id\": \"call_o9udf3EVOWiV4Iupktpbpofk\",\n",
    "            \"type\": \"tool_call\",\n",
    "        }\n",
    "    ],\n",
    "    usage_metadata={\"input_tokens\": 68, \"output_tokens\": 17, \"total_tokens\": 85},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is LangGraph?\"\n",
    "# Initiating the messages list with the user query as the first element\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = augmented_llm.invoke(messages)\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is LangGraph?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_s3fvcRYFX4EElyIEXjWZJU2p', 'function': {'arguments': '{\"query\":\"What is LangGraph?\"}', 'name': 'langgraph_query_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 80, 'total_tokens': 176, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-2024-12-17', 'system_fingerprint': 'fp_688960522e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c8550fd3-cbc3-4a97-990b-c8be95134bdd-0', tool_calls=[{'name': 'langgraph_query_tool', 'args': {'query': 'What is LangGraph?'}, 'id': 'call_s3fvcRYFX4EElyIEXjWZJU2p', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 96, 'total_tokens': 176, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
       " ToolMessage(content='==DOCUMENT 1==\\nWhy LangGraph?¶\\nLLM applications¶\\nLLMs make it possible to embed intelligence into a new class of applications. There are many patterns for building applications that use LLMs. Workflows have scaffolding of predefined code paths around LLM calls. LLMs can direct the control flow through these predefined code paths, which some consider to be an \"agentic system\". In other cases, it\\'s possible to remove this scaffolding, creating autonomous agents that can plan, take actions via tool calls, and directly respond to the feedback from their own actions with further actions.\\n\\nWhat LangGraph provides¶\\nLangGraph provides low-level supporting infrastructure that sits underneath any workflow or agent. It does not abstract prompts or architecture, and provides three central benefits:\\nPersistence¶\\nLangGraph has a persistence layer, which offers a number of benefits:\\n\\nMemory: LangGraph persists arbitrary aspects of your application\\'s state, supporting memory of conversations and other updates within and across user interactions;\\nHuman-in-the-loop: Because state is checkpointed, execution can be interrupted and resumed, allowing for decisions, validation, and corrections via human input.\\n\\nStreaming¶\\nLangGraph also provides support for streaming workflow / agent state to the user (or developer) over the course of execution. LangGraph supports streaming of both events (such as feedback from a tool call) and tokens from LLM calls embedded in an application.\\nDebugging and Deployment¶\\nLangGraph provides an easy onramp for testing, debugging, and deploying applications via LangGraph Platform. This includes Studio, an IDE that enables visualization, interaction, and debugging of workflows or agents. This also includes numerous options for deployment. \\n\\n        Was this page helpful?\\n      \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\nComments\\n\\n==DOCUMENT 2==\\nFAQ¶\\nCommon questions and their answers!\\nDo I need to use LangChain to use LangGraph? What’s the difference?¶\\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\\nHow is LangGraph different from other agent frameworks?¶\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.\\nDoes LangGraph impact the performance of my app?¶\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\nIs LangGraph open source? Is it free?¶\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\nHow are LangGraph and LangGraph Platform different?¶\\nLangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangGraph Platform is a service for deploying and scaling LangGraph applications, with an opinionated API for building agent UXs, plus an integrated developer studio.\\n\\nFeatures\\nLangGraph (open source)\\nLangGraph Platform\\n\\nDescription\\nStateful orchestration framework for agentic applications\\nScalable infrastructure for deploying LangGraph applications\\n\\nSDKs\\nPython and JavaScript\\nPython and JavaScript\\n\\nHTTP APIs\\nNone\\nYes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant\\n\\nStreaming\\nBasic\\nDedicated mode for token-by-token messages\\n\\nCheckpointer\\nCommunity contributed\\nSupported out-of-the-box\\n\\nPersistence Layer\\nSelf-managed\\nManaged Postgres with efficient storage\\n\\nDeployment\\nSelf-managed\\n• Cloud SaaS  • Free self-hosted  • Enterprise (BYOC or paid self-hosted)\\n\\nScalability\\nSelf-managed\\nAuto-scaling of task queues and servers\\n\\nFault-tolerance\\nSelf-managed\\nAutomated retries\\n\\nConcurrency Control\\nSimple threading\\nSupports double-texting\\n\\nScheduling\\nNone\\nCron scheduling\\n\\nMonitoring\\nNone\\nIntegrated with LangSmith for observability\\n\\nIDE integration\\nLangGraph Studio\\nLangGraph Studio\\n\\nWhat are my deployment options for LangGraph Platform?¶\\nWe currently have the following deployment options for LangGraph applications:\\n\\n\\u200dSelf-Hosted Lite: A free (up to 1M nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner. This version requires a LangSmith API key and logs all usage to LangSmith. Fewer features are available than in paid plans.\\nCloud SaaS: Fully managed and hosted as part of LangSmith, with automatic updates and zero maintenance.\\n\\u200dBring Your Own Cloud (BYOC): Deploy LangGraph Platform within your VPC, provisioned and run as a service. Keep data in your environment while outsourcing the management of the service.\\nSelf-Hosted Enterprise: Deploy LangGraph entirely on your own infrastructure.\\n\\nIs LangGraph Platform open source?¶\\nNo. LangGraph Platform is proprietary software.\\nThere is a free, self-hosted version of LangGraph Platform with access to basic features. The Cloud SaaS deployment option is free while in beta, but will eventually be a paid service. We will always give ample notice before charging for a service and reward our early adopters with preferential pricing. The Bring Your Own Cloud (BYOC) and Self-Hosted Enterprise options are also paid services. Contact our sales team to learn more.\\nFor more information, see our LangGraph Platform pricing page.\\nDoes LangGraph work with LLMs that don\\'t support tool calling?¶\\nYes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.\\nDoes LangGraph work with OSS LLMs?¶\\nYes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don\\'t. But tool calling is not necessary (see this section) so you can totally use LangGraph with OSS LLMs.\\nCan I use LangGraph Studio without logging to LangSmith¶\\nYes! You can use the development version of LangGraph Server to run the backend locally.\\nThis will connect to the studio frontend hosted as part of LangSmith.\\nIf you set an environment variable of LANGSMITH_TRACING=false then no traces will be sent to LangSmith.\\n\\n        Was this page helpful?\\n      \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\nComments\\n\\n==DOCUMENT 3==\\nLangGraph Platform¶\\nOverview¶\\nLangGraph Platform is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework.\\nThe LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications:\\n\\nLangGraph Server: The server defines an opinionated API and architecture that incorporates best practices for deploying agentic applications, allowing you to focus on building your agent logic rather than developing server infrastructure.\\nLangGraph Studio: LangGraph Studio is a specialized IDE that can connect to a LangGraph Server to enable visualization, interaction, and debugging of the application locally.\\nLangGraph CLI: LangGraph CLI is a command-line interface that helps to interact with a local LangGraph\\nPython/JS SDK: The Python/JS SDK provides a programmatic way to interact with deployed LangGraph Applications.\\nRemote Graph: A RemoteGraph allows you to interact with any deployed LangGraph application as though it were running locally.\\n\\nThe LangGraph Platform offers a few different deployment options described in the deployment options guide.\\nWhy Use LangGraph Platform?¶\\nLangGraph Platform handles common issues that arise when deploying LLM applications to production, allowing you to focus on agent logic instead of managing server infrastructure.\\n\\nStreaming Support: As agents grow more sophisticated, they often benefit from streaming both token outputs and intermediate states back to the user. Without this, users are left waiting for potentially long operations with no feedback. LangGraph Server provides multiple streaming modes optimized for various application needs.\\n\\nBackground Runs: For agents that take longer to process (e.g., hours), maintaining an open connection can be impractical. The LangGraph Server supports launching agent runs in the background and provides both polling endpoints and webhooks to monitor run status effectively.\\n\\nSupport for long runs: Vanilla server setups often encounter timeouts or disruptions when handling requests that take a long time to complete. LangGraph Server’s API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes.\\n\\nHandling Burstiness: Certain applications, especially those with real-time user interaction, may experience \"bursty\" request loads where numerous requests hit the server simultaneously. LangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads.\\n\\nDouble Texting: In user-driven applications, it’s common for users to send multiple messages rapidly. This “double texting” can disrupt agent flows if not handled properly. LangGraph Server offers built-in strategies to address and manage such interactions.\\n\\nCheckpointers and Memory Management: For agents needing persistence (e.g., conversation memory), deploying a robust storage solution can be complex. LangGraph Platform includes optimized checkpointers and a memory store, managing state across sessions without the need for custom solutions.\\n\\nHuman-in-the-loop Support: In many applications, users require a way to intervene in agent processes. LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.\\n\\nBy using LangGraph Platform, you gain access to a robust, scalable deployment solution that mitigates these challenges, saving you the effort of implementing and maintaining them manually. This allows you to focus more on building effective agent behavior and less on solving deployment infrastructure issues.\\n\\n        Was this page helpful?\\n      \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\nComments', name='langgraph_query_tool', tool_call_id='call_s3fvcRYFX4EElyIEXjWZJU2p')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"langgraph_query_tool\": langgraph_query_tool}[\n",
    "        tool_call[\"name\"].lower()\n",
    "    ]\n",
    "tool_msg = selected_tool.invoke(tool_call)\n",
    "messages.append(tool_msg)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangGraph is an open-source orchestration framework for building and running “agentic” LLM applications. It sits beneath the high-level application logic (such as prompts and workflows) and provides three major capabilities:\\n\\n• Persistence: It stores your application’s state—such as conversation history or task progress—so it can be resumed or audited later, enabling memory and human-in-the-loop interactions.  \\n• Streaming: It supports streaming tokens from the model output as well as streaming events (like tool feedback) so users get real-time updates.  \\n• Debugging and Deployment: It offers an easy path for testing, monitoring, and deploying these applications via LangGraph Platform, which includes an IDE (LangGraph Studio) and infrastructure for scaling to production.\\n\\nWith LangGraph, you can build anything from simple predefined workflows to fully autonomous agents that plan, take actions with code or tools, and adapt in response to their own outputs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 388, 'prompt_tokens': 2237, 'total_tokens': 2625, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-2024-12-17', 'system_fingerprint': 'fp_688960522e', 'finish_reason': 'stop', 'logprobs': None}, id='run-066574dc-8d38-4153-8b14-385dca184eab-0', usage_metadata={'input_tokens': 2237, 'output_tokens': 388, 'total_tokens': 2625, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
