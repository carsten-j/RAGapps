{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = model.tokenize([\"walker walked a long walk\"])\n",
    "tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.convert_ids_to_tokens(tokenized_data[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer consists of multiple stack modules. Tokens are an input\n",
    "# of the first one, so we can ignore the rest.\n",
    "first_module = model._first_module()\n",
    "first_module.auto_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = first_module.auto_model.embeddings\n",
    "embeddings\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.has_mps else \"cpu\"\n",
    ")  # Use MPS for Apple, CUDA for others, or fallback to CPU\n",
    "\n",
    "first_sentence = \"vector search optimization\"\n",
    "second_sentence = \"we learn to apply vector search optimization\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tokenize both texts\n",
    "    first_tokens = model.tokenize([first_sentence])\n",
    "    second_tokens = model.tokenize([second_sentence])\n",
    "\n",
    "    # Get the corresponding embeddings\n",
    "    first_embeddings = embeddings.word_embeddings(first_tokens[\"input_ids\"].to(device))\n",
    "    second_embeddings = embeddings.word_embeddings(\n",
    "        second_tokens[\"input_ids\"].to(device)\n",
    "    )\n",
    "\n",
    "first_embeddings.shape, second_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = (\n",
    "    util.cos_sim(first_embeddings.squeeze(), second_embeddings.squeeze()).cpu().numpy()\n",
    ")  # Move the tensor to the CPU and convert to a NumPy array\n",
    "\n",
    "px.imshow(\n",
    "    distances,\n",
    "    x=model.tokenizer.convert_ids_to_tokens(second_tokens[\"input_ids\"][0]),\n",
    "    y=model.tokenizer.convert_ids_to_tokens(first_tokens[\"input_ids\"][0]),\n",
    "    text_auto=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Visualizing the input embeddings\n",
    "\n",
    "\n",
    "token_embeddings = (\n",
    "    first_module.auto_model.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    ")\n",
    "token_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = first_module.tokenizer.get_vocab()\n",
    "sorted_vocabulary = sorted(\n",
    "    vocabulary.items(),\n",
    "    key=lambda x: x[1],  # uses the value of the dictionary entry\n",
    ")\n",
    "sorted_tokens = [token for token, _ in sorted_vocabulary]\n",
    "random.choices(sorted_tokens, k=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, metric=\"cosine\", random_state=42)\n",
    "tsne_embeddings_2d = tsne.fit_transform(token_embeddings)\n",
    "tsne_embeddings_2d.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_colors = []\n",
    "for token in sorted_tokens:\n",
    "    if token[0] == \"[\" and token[-1] == \"]\":\n",
    "        token_colors.append(\"red\")\n",
    "    elif token.startswith(\"##\"):\n",
    "        token_colors.append(\"blue\")\n",
    "    else:\n",
    "        token_colors.append(\"green\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "scatter = go.Scattergl(\n",
    "    x=tsne_embeddings_2d[:, 0],\n",
    "    y=tsne_embeddings_2d[:, 1],\n",
    "    text=sorted_tokens,\n",
    "    marker=dict(color=token_colors, size=3),\n",
    "    mode=\"markers\",\n",
    "    name=\"Token embeddings\",\n",
    ")\n",
    "\n",
    "fig = go.FigureWidget(\n",
    "    data=[scatter],\n",
    "    layout=dict(\n",
    "        width=600,\n",
    "        height=900,\n",
    "        margin=dict(l=0, r=0),\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# ## Output token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedding = model.encode([\"walker walked a long walk\"])\n",
    "output_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_token_embeddings = model.encode(\n",
    "    [\"walker walked a long walk\"], output_value=\"token_embeddings\"\n",
    ")\n",
    "output_token_embeddings[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"vector search optimization\"\n",
    "second_sentence = \"we learn to apply vector search optimization\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    first_tokens = model.tokenize([first_sentence])\n",
    "    second_tokens = model.tokenize([second_sentence])\n",
    "\n",
    "    first_embeddings = model.encode([first_sentence], output_value=\"token_embeddings\")\n",
    "    second_embeddings = model.encode([second_sentence], output_value=\"token_embeddings\")\n",
    "\n",
    "distances = util.cos_sim(first_embeddings[0], second_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "    distances.cpu().numpy(),  # Move the tensor to CPU and convert to a NumPy array\n",
    "    x=model.tokenizer.convert_ids_to_tokens(second_tokens[\"input_ids\"][0]),\n",
    "    y=model.tokenizer.convert_ids_to_tokens(first_tokens[\"input_ids\"][0]),\n",
    "    text_auto=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
